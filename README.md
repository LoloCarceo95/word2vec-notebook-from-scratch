# Word2Vec from Scratch (PyTorch)

Implementaci√≥n **desde cero** de Word2Vec (Skip-gram con Negative Sampling) en PyTorch, con un enfoque did√°ctico y paso a paso.

El objetivo de este proyecto es **entender realmente** c√≥mo funcionan los embeddings de palabras, sin usar implementaciones ‚Äúcaja negra‚Äù.

---

##  Contenido del notebook

El notebook incluye:

- Limpieza y tokenizaci√≥n del corpus (WikiText-2)
- Construcci√≥n del vocabulario y filtrado de palabras raras
- Subsampling de palabras frecuentes
- Generaci√≥n del corpus num√©rico
- Dataset Skip-gram
- Negative Sampling
- Definici√≥n del modelo Word2Vec en PyTorch
- Entrenamiento desde cero
- Evaluaci√≥n de embeddings:
  - similitud coseno
  - analog√≠as (`king - man + woman ‚âà queen`)
- Visualizaci√≥n de embeddings en 2D y 3D (PCA)

---

##  Conceptos clave

- Skip-gram
- Negative Sampling
- Embeddings como par√°metros entrenables
- Distribuci√≥n de muestreo negativa `P(w) ‚àù f(w)^(3/4)`
- Geometr√≠a sem√°ntica en espacios vectoriales

---

## üõ†Ô∏è Tecnolog√≠as usadas

- Python 3
- PyTorch
- NumPy
- scikit-learn
- Matplotlib / Plotly
- Jupyter Notebook

---

##  Uso

1. Clona el repositorio:
   ```bash
   git clone https://github.com/LoloCarceo95/word2vec-notebook-from-scratch.git
